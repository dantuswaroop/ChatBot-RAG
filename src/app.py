from pdf_loader import extract_text_from_pdfs
from chunker import chunk_text, chunk_text_with_metadata
from embedder import build_faiss_index
from retriever import load_faiss_index, retrieve
from generator import generate_answer, generate_detailed_answer
import os
from pathlib import Path

PDF_DIR = "data"
EMBEDDING_DIR = "embeddings"

def run_build_pipeline():
    print("ğŸ“„ Extracting PDFs with page information...")
    raw_docs = extract_text_from_pdfs(PDF_DIR)

    if not raw_docs:
        print("âŒ No valid PDFs found or processed!")
        print("ğŸ’¡ Make sure you have non-empty PDF files in the 'data/' directory.")
        print("ğŸ“‹ Current data directory contents:")
        data_path = Path(PDF_DIR)
        if data_path.exists():
            for pdf_file in data_path.glob("*.pdf"):
                size = os.path.getsize(pdf_file)
                status = "âœ… Valid" if size > 100 else "âŒ Empty/Invalid"
                print(f"   {pdf_file.name}: {size} bytes - {status}")
        return

    all_chunks = []
    all_meta = []

    for filename, text, page_metadata in raw_docs:
        print(f"âœ‚ï¸ Chunking {filename} with page tracking...")
        chunks_with_meta = chunk_text_with_metadata(text, page_metadata)
        
        for chunk_info in chunks_with_meta:
            all_chunks.append(chunk_info["text"])
            all_meta.append({
                "source": filename,
                "text": chunk_info["text"],
                "pages": chunk_info["pages"],
                "char_start": chunk_info["char_start"],
                "char_end": chunk_info["char_end"]
            })

    if not all_chunks:
        print("âŒ No text chunks created! PDFs may not contain extractable text.")
        return

    os.makedirs(EMBEDDING_DIR, exist_ok=True)
    print("ğŸ”— Building FAISS index...")
    build_faiss_index(all_chunks, all_meta, EMBEDDING_DIR)

    print("âœ… Embedding complete and index saved.")


def ask_question(query: str, top_k: int = 5):
    print("ğŸ” Retrieving relevant context...")
    index, metadata = load_faiss_index(EMBEDDING_DIR)
    relevant_chunks = retrieve(query, index, metadata, k=top_k)

    print("ğŸ’¬ Generating detailed answer with references...")
    answer = generate_detailed_answer(query, relevant_chunks)
    
    # Add source summary at the end
    sources_summary = "\n\nğŸ“š **Sources Used:**\n"
    unique_sources = {}
    for chunk in relevant_chunks:
        source = chunk.get('source', 'Unknown')
        pages = chunk.get('pages', [])
        if source not in unique_sources:
            unique_sources[source] = set()
        unique_sources[source].update(pages)
    
    for source, pages in unique_sources.items():
        sorted_pages = sorted(list(pages))
        if sorted_pages:
            sources_summary += f"â€¢ {source}: Pages {', '.join(map(str, sorted_pages))}\n"
        else:
            sources_summary += f"â€¢ {source}: Page information unavailable\n"
    
    return answer + sources_summary


def interactive_chat():
    """
    Interactive chat mode - continuously waits for user questions
    and provides detailed answers with references until the program is killed (Ctrl+C).
    """
    print("ğŸ¤– Enhanced RAG PDF Chat Assistant")
    print("=" * 60)
    print("Ask detailed questions and get comprehensive answers with:")
    print("â€¢ Specific examples from documents")
    print("â€¢ Page number references")
    print("â€¢ Multiple document sources")
    print("Type your questions and press Enter. Use Ctrl+C to exit.")
    print("=" * 60)
    
    # Check if embeddings exist
    if not os.path.exists(os.path.join(EMBEDDING_DIR, "index.faiss")):
        print("âŒ No embeddings found! Please run with --build first to index your PDFs.")
        return
    
    try:
        while True:
            # Get user input
            print("\n" + "â”€" * 50)
            user_query = input("â“ Your question: ").strip()
            
            # Skip empty queries
            if not user_query:
                print("Please enter a valid question.")
                continue
            
            # Process the query
            try:
                print("ğŸ”„ Processing your question...")
                response = ask_question(user_query, top_k=7)  # Get more context for detailed answers
                print(f"\nğŸ§  **Detailed Answer:**\n{response}")
                
                # Optional: Show relevance information
                print(f"\nğŸ’¡ *Tip: The answer above includes references to specific pages and documents.*")
                
            except Exception as e:
                print(f"âŒ Error processing your question: {str(e)}")
                print("Please try again with a different question.")
                
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Thank you for using Enhanced RAG PDF Chat Assistant. Goodbye!")
    except EOFError:
        print("\n\nğŸ‘‹ Chat session ended. Goodbye!")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Ask questions from your local PDFs using RAG + Ollama.")
    parser.add_argument("--build", action="store_true", help="Run the indexing pipeline on PDFs.")
    parser.add_argument("--ask", type=str, help="Ask a single question based on the indexed PDFs.")
    parser.add_argument("--chat", action="store_true", help="Start interactive chat mode (default if no other option is provided).")

    args = parser.parse_args()

    if args.build:
        run_build_pipeline()
    elif args.ask:
        response = ask_question(args.ask)
        print(f"\nğŸ§  Answer:\n{response}")
    elif args.chat:
        interactive_chat()
    else:
        # Default to interactive chat mode if no arguments provided
        interactive_chat()
